{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Demonstration with PyTorch\n",
    "\n",
    "\n",
    "In this example, we use the breast cancer dataset to show how you can use PyTorch neural networks for a binary classification problem. The goal is to predict breast cancer diagnosis (0 = not diagnosed, 1 = diagnosed) based on cellular features extracted from breast mass images.\n",
    "\n",
    "## Learning Objectives:\n",
    "- Set up and verify PyTorch environment\n",
    "- Load and preprocess medical data for deep learning\n",
    "- Build a simple neural network for binary classification\n",
    "- Train and evaluate the model using appropriate classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "First, let's verify our PyTorch installation and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn for preprocessing and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Breast Cancer Dataset\n",
    "\n",
    "For this demo, we'll use the breast cancer dataset which contains features computed from digitised images of breast masses. Each sample describes characteristics of cell nuclei present in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset from local CSV file\n",
    "cancer_data = pd.read_csv('./breast_cancer.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {cancer_data.shape}\")\n",
    "print(f\"\\nColumn names: {cancer_data.columns.tolist()}\")\n",
    "print(f\"\\nDiagnosis distribution:\")\n",
    "print(cancer_data['diagnosis'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(cancer_data['diagnosis'].value_counts(normalize=True))\n",
    "\n",
    "# Show first few rows\n",
    "cancer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Preprocessing step is crucial for neural network training. We separate our features (cellular characteristics) from our target variable (diagnosis), split the data into training and testing sets to evaluate model performance, and apply standardisation to normalise all features to have zero mean and unit variance. This standardisation ensures that no single feature dominates the learning process due to its scale. Finally, we convert our data to PyTorch tensors and create DataLoaders that will efficiently batch and shuffle our data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = cancer_data.drop('diagnosis', axis=1).values\n",
    "y = cancer_data['diagnosis'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardise features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Training set class distribution: {np.bincount(y_train.astype(int))}\")\n",
    "print(f\"Test set class distribution: {np.bincount(y_test.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a tensor?**\n",
    "\n",
    "A tensor is a mathematical object that generalises familiar concepts like numbers, vectors, and matrices. Think of it as a container for data that can have different dimensions - a single number (0D), a list of numbers (1D), a table of numbers (2D), or even higher-dimensional arrangements. In machine learning, tensors are fundamental because they provide a unified way to represent and manipulate the complex, multi-dimensional data that neural networks process, from simple input features to the intricate weight matrices that define how networks learn and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Neural Network Model\n",
    "\n",
    "Create a simple feedforward neural network for breast cancer diagnosis prediction.\n",
    "\n",
    "**Neural Network Architecture Explanation:**\n",
    "\n",
    "Our `BreastCancerNet` is a feedforward neural network with four fully connected (linear) layers that progressively reduce the dimensionality: input → 64 → 32 → 16 → 1 neuron. This architecture creates a funnel-like structure that learns to extract increasingly abstract features from the cellular characteristics to predict diagnosis. We use ReLU (Rectified Linear Unit) activation functions between layers to introduce non-linearity, allowing the network to learn complex patterns. Dropout layers (20% probability) are included to prevent overfitting by randomly setting some neurons to zero during training, forcing the network to be more robust and generalisable. The final layer uses a sigmoid activation function to output probabilities between 0 and 1 for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BreastCancerNet, self).__init__()\n",
    "        \n",
    "        # Define layers with progressively decreasing dimensions\n",
    "        self.fc1 = nn.Linear(input_dim, 64)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(64, 32)         # Second hidden layer  \n",
    "        self.fc3 = nn.Linear(32, 16)         # Third hidden layer\n",
    "        self.fc4 = nn.Linear(16, 1)          # Output layer\n",
    "        \n",
    "        # Activation and regularisation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()         # For binary classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.relu(self.fc1(x))  # Apply ReLU activation to first layer\n",
    "        x = self.dropout(x)         # Apply dropout for regularization\n",
    "        x = self.relu(self.fc2(x))  # Apply ReLU activation to second layer\n",
    "        x = self.dropout(x)         # Apply dropout for regularization\n",
    "        x = self.relu(self.fc3(x))  # Apply ReLU activation to third layer\n",
    "        x = self.fc4(x)             # Final linear transformation\n",
    "        x = self.sigmoid(x)         # Sigmoid activation for binary classification\n",
    "        return x\n",
    "\n",
    "# Initialise model\n",
    "input_dim = X_train.shape[1]  # Number of features (30 in our case)\n",
    "model = BreastCancerNet(input_dim=input_dim).to(device)\n",
    "\n",
    "# Display model architecture\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup\n",
    "\n",
    "Configure loss function, optimiser, and training parameters.\n",
    "\n",
    "**Key Components Explained:**\n",
    "\n",
    "**BCE (Binary Cross-Entropy):** Our loss function that measures the difference between predicted probabilities and actual binary labels. BCE is ideal for binary classification problems as it penalises confident wrong predictions more heavily and provides smooth gradients for optimisation.\n",
    "\n",
    "**Adam Optimiser:** An adaptive learning rate optimiser that combines the benefits of momentum and RMSprop. Adam automatically adjusts the learning rate for each parameter individually, making it very effective for training neural networks with minimal hyperparameter tuning.\n",
    "\n",
    "**Training Functions:** The `train_epoch` function performs forward propagation (calculating predictions), computes the loss, and uses backpropagation to update model weights. The `validate` function evaluates model performance on unseen data without updating weights, helping us monitor for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in dataloader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Train the neural network and monitor performance by calling the previous functions per training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualise Training Progress\n",
    "\n",
    "**What we expect to see:** \n",
    "Both training and validation loss should decrease over time, indicating the model is learning. Ideally, both curves should converge to a similar low value. If the training loss continues decreasing while validation loss increases, this indicates overfitting. If both curves plateau at a high value, the model may be underfitting and need more complexity or different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Evaluate the model's performance on the test set using classification metrics.\n",
    "\n",
    "**Metrics Explained:**\n",
    "\n",
    "**Accuracy:** The proportion of correct predictions out of total predictions. For medical diagnosis, high accuracy is important but not the only consideration.\n",
    "\n",
    "**Precision:** The proportion of positive predictions that were actually correct. In medical context, this represents how often we correctly identify cancer when we predict it.\n",
    "\n",
    "**Recall (Sensitivity):** The proportion of actual positive cases that were correctly identified. This is crucial in medical diagnosis as we want to catch as many cancer cases as possible.\n",
    "\n",
    "**F1-Score:** The harmonic mean of precision and recall, providing a balanced measure when both are important.\n",
    "\n",
    "**Confusion Matrix:** Shows the breakdown of correct and incorrect predictions for each class, helping identify if the model has bias towards certain predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_device = X_test_tensor.to(device)\n",
    "    prediction_probs = model(X_test_device).squeeze().cpu().numpy()\n",
    "    predictions = (prediction_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, predictions, target_names=['Benign', 'Malignant']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Benign', 'Malignant'], \n",
    "            yticklabels=['Benign', 'Malignant'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC-style visualization: Predicted probabilities distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(prediction_probs[y_test == 0], bins=30, alpha=0.7, label='Benign', color='blue')\n",
    "plt.hist(prediction_probs[y_test == 1], bins=30, alpha=0.7, label='Malignant', color='red')\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Predicted Probabilities by True Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Model to the local folder\n",
    "\n",
    "You would typically save the model to a local file for recall at another time for further training or predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model as a .pth file in the current directory\n",
    "model_path = 'breast_cancer_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'input_dim': X_train.shape[1],\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses\n",
    "}, model_path)\n",
    "\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the model as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the models parameters \n",
    "checkpoint = torch.load('./breast_cancer_model.pth', weights_only=False)\n",
    "\n",
    "# Grab the input dimensions and initialise the model\n",
    "input_dim = checkpoint['input_dim']\n",
    "model = BreastCancerNet(input_dim=input_dim)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set to evaluation mode for predictions (it switches off the drop-out layers so all neurons are active)\n",
    "# If you are training further, forgo this step\n",
    "model.eval()\n",
    "\n",
    "# Move to appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential improvements:\n",
    "- Experiment with different network architectures\n",
    "- Try different optimisation techniques\n",
    "- Implement cross-validation for more robust evaluation\n",
    "- Add feature importance analysis\n",
    "- Experiment with different threshold values for classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
